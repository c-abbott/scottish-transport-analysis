{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: *Azfarul Islam* and *Callum Abbott*\n",
    "\n",
    "**Date**: December, 2020\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš¦ Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primary method of reporting road traffic by the Department for Transport (DfT) involves estimating *Average Annual Daily Flow* (AADF) of traffic on UK roads. Counts are manually performed at pre-determined locations (called 'count points'), periodically. Typically, these counts are collected over a 12-hour period, only on weekdays, between March to October. There may be inconsistencies in which roads are counted, year on year.\n",
    "\n",
    "Utilising the public dataset under an [Open Government License](http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/), we explore it to understand and present our insights on traffic trends in Scotland (because we are at the University of Edinburgh, obviously) covering a period from 2000 to 2019.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ Approach\n",
    "\n",
    "Before we begin deriving insights from our primary dataset, we decided to perform some basic exploratory data analysis (EDA) in order to understand what we are dealing with. This would also help us confirm which questions we thought would be interesting to answer, and consequently decide what appropriate transformations of the dataset would be deemed necessary.\n",
    "\n",
    "Before we started any code development, we summarised the outcomes we wished to achieve for each visualisation. This was followed with a breakdown of steps we would need to take to arrive at our planned visualisation (e.g. figure formatting, external data, pivotting, etc.).\n",
    "\n",
    "After these steps, if we found any alternative avenues of interest, we either added or pivoted towards those as appropriate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ² *There be dragons (i.e. code begins)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import folium\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import json\n",
    "import os\n",
    "import descartes\n",
    "from shapely.geometry import Point, Polygon\n",
    "from datetime import datetime\n",
    "\n",
    "# Setting Pandas display parameters\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§¹ðŸ—º Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick set of EDA actions were performed where we simply observed the first few rows of data and referenced them against the descriptions for each column to ensure elements were as expected. We also utilised the `.info()` and `.describe()` methods for the dataframe to see the types of columns, as well as exploring summary statistics. \n",
    "\n",
    "Besides the specific questions we wanted to answer, this also helped us re-affirm any columns that we would not need to use.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('data/dft_rawcount_region_id_3.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # Quickly viewing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all') # Basic EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸ§¹ Tidying our data\n",
    "\n",
    "Inspired by [TidyVerse](https://style.tidyverse.org) principles from R (thank you, *Hadley Wickham*), we decided to create a reduced data-frame which would only keep the signal and remove the noise, as it were. \n",
    "\n",
    "Here follows a set of transformations made to render our tidy dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unnecessary columns\n",
    "\n",
    "Once we were mostly certain about the questions we wanted to answer, we could comfortably drop a large number of columns from the original dataframe. In some cases there was redundant data (such as `region_id` which contained only one value, or all the different types of HGV) in other cases, we simply did not need to keep that data. For HGVs, we only retained the `all_hgvs` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy = df.copy() # Tidy dataset storage\n",
    "\n",
    "# Cols to drop\n",
    "rm_cols = ['region_id', 'region_name', 'local_authority_id', 'road_type',\n",
    "           'start_junction_road_name', 'end_junction_road_name',\n",
    "           'year', 'easting', 'northing', 'link_length_miles', 'link_length_km', 'hgvs_2_rigid_axle',\n",
    "           'hgvs_3_rigid_axle', 'hgvs_3_or_4_articulated_axle',\n",
    "           'hgvs_4_or_more_rigid_axle', 'hgvs_5_articulated_axle',\n",
    "           'hgvs_6_articulated_axle']\n",
    "\n",
    "# Drop cols\n",
    "df_tidy.drop(rm_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Vehicle Labels \n",
    "\n",
    "We wanted to have more interpretable labels for each vehicle type, so we renamed them appropriately. This would make labelling in plots simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = ['bikes', 'motorcycles', 'cars', 'buses', 'lgvs', 'hgvs'] # More interpretable labels\n",
    "name_mappings = dict(zip(df_tidy.columns[8:-1], new_names)) # Creating dict of label mappings\n",
    "df_tidy.rename(columns=name_mappings, inplace=True) # Renaming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Transformations\n",
    "\n",
    "We noticed that there was a column called `all_motor_vehicles` which was a sum of counts per row for all vehicles save for bikes; we wanted to have a complete count which we called `all_vehicles`. \n",
    "\n",
    "As we knew we had interest in the type of road based on its letter designation, we utilised an anonymous function to extract that into a new column called `road_type`. Note that we dropped their original `road_type` column as we were not interested in it.]\n",
    "\n",
    "Finally, we converted `count_date` to a `DateTime64` object, as we would utilise dates in a few of our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_motor_vehicles --> all_vehicles (include bikes)\n",
    "df_tidy['all_vehicles'] = df_tidy['bikes'] + df_tidy['motorcycles'] + df_tidy['cars'] + df_tidy['buses'] + \\\n",
    "                          df_tidy['lgvs'] + df_tidy['hgvs']\n",
    "\n",
    "# New column for specifying whether road is: M, A, B, C, U\n",
    "df_tidy = df_tidy.assign(\n",
    "            road_type = lambda dataframe: dataframe['road_name'].map(lambda road_name: road_name[0]))\n",
    "\n",
    "# Convert count_date to datetime format\n",
    "df_tidy['count_date'] = pd.to_datetime(df['count_date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Population Data\n",
    "\n",
    "We acquired population data covering mid-2019 estimates from [National Records of Scotland](https://www.nrscotland.gov.uk/statistics-and-data/statistics/statistics-by-theme/population/population-estimates/mid-year-population-estimates/mid-2019). Due to the Pandas-unfriendly structure of their dataset, we simply adapted it into a tiny dataset covering only the estimated populations for each local authority in Scotland.\n",
    "\n",
    "We had to convert the `population` column into integers, and had to change the name of one of the local authorities (called, interchangeably: *Na h-Eileanan Siar*, *Comhairle nan Eilean Siar*, and *Eilean Siar*) to ensure there was parity across all dataframes. We merged the population data with our main dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **Please note warning in code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in population statistics\n",
    "# Line(s) 5: National Records of Scotland\n",
    "# URL: https://www.nrscotland.gov.uk/statistics-and-data/statistics/statistics-by-theme/population/population-estimates/mid-year-population-estimates/mid-2019\n",
    "# Accessed on 7th December 2020.\n",
    "pop_df = pd.read_csv('data/scotland-mid-year-pop-est-19.csv', header=None)\n",
    "\n",
    "# Rename columns\n",
    "pop_df.rename(columns={0: 'local_authority_name', 1: 'population'}, inplace=True)\n",
    "\n",
    "# Convert population to int\n",
    "pop_df.population = pop_df.population.str.replace(',', '')\n",
    "pop_df.population = pop_df.population.astype('int') \n",
    "\n",
    "# String replaces to avoid join issues\n",
    "pop_df.local_authority_name = pop_df.local_authority_name.str.replace('Na h-Eileanan Siar', 'Eilean Siar')\n",
    "df_tidy.local_authority_name = df_tidy.local_authority_name.str.replace('Comhairle nan Eilean Siar', 'Eilean Siar')\n",
    "df_tidy.local_authority_name = df_tidy.local_authority_name.str.replace('&', 'and')\n",
    "\n",
    "# Overwrite df_tidy with full outer join with population data`\n",
    "# WARNING: this operation is idempotent so repeated runs will keep adding a population column\n",
    "# Must restart kernel if this needs to be run again \n",
    "df_tidy = pd.merge(df_tidy, pop_df, on='local_authority_name', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tidy.head(100) # Final check of tidy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can happily confirm that our tidy dataframe has no missing (`NaN`) values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸš² Bikes observed per 10k residents per local authority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to investigate data around bikes, and thought that bike observations per local authority name would be interesting to start with. However, we need to establish the following caveats:\n",
    "1. Count data of bikes is assumed to be representative of the prevalence of bikes in the authority\n",
    "2. A bike count within a local authority maps to that bike actually belonging to that authority\n",
    "3. Double counting effects are ignored\n",
    "\n",
    "With regards to caveats 1 and 2, we think those are reasonable assumptions since bikes are used for micro-mobility in the majority of cases.\n",
    "\n",
    "Our visualisation is in the form of a horizontal bar chart ordered by the observations per capita. We later revised this to observations per 10,000 residents for scaling purposes later on.\n",
    "\n",
    "We noticed that the highest number of observations was for the *Orkney Islands* which we know to be one of the least densely populated authorities in Scotland. That prompted us to explore any relationships between bike observations and population. Initially, we thought about embedding population figures directly on the bar chart but realised that would be visually engaging. For our next iteration, we focused on a 'choropleth' map  to incorporate a interactive dimension to this data visualisation.\n",
    "\n",
    "## âš ï¸ TODO:\n",
    "* Use only 2019 bikes observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple helper function to perform \n",
    "# divide one pd column by another\n",
    "def div_series(df):\n",
    "    return df['bikes'].sum() * 10000 / float(df['population'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting relevant data for bike viz\n",
    "bikes_cols = ['local_authority_name', 'bikes', 'population']\n",
    "df_bikes = df_tidy[bikes_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped data frame\n",
    "bikes_per_10k_residents = df_bikes.groupby(['local_authority_name'], \n",
    "                                           as_index=True).apply(div_series).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar plot of bikes per 10k residents\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (10, 25))\n",
    "pal = sns.light_palette(\"seagreen\", n_colors=40, reverse=True)\n",
    "sns.barplot(x = bikes_per_10k_residents, y = bikes_per_10k_residents.index, palette=pal)\n",
    "ax.set_title(\"DfT 2000-2019 Cycle Counts per 10k Residents \\nper Scottish Local Authority\",\n",
    "            pad=10, loc = \"left\", fontdict={'weight':'heavy', 'color':'black'}, fontsize=25)\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_xlabel(\"Cycles counts per 10k residents\", fontsize=25, labelpad=10)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.figtext(x=.75, y=0, \n",
    "            s='Source: DfT Road Traffic Statistics', \n",
    "            fontdict={'style':'oblique', 'color':'black'})\n",
    "plt.grid(which='major', axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ðŸšµðŸ½â€â™€ï¸ Bikes observed per 10k residents per local authority - projected on a map of Scotland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this more complex visualisation, we needed to acquire [topographical data](https://github.com/martinjc/UK-GeoJSON) (in TopoJSON format) so that we could that we convert into a map using `GeoPandas`, with a focus on authority borders. We played around with the starting position when the map is loaded due to the fact that Scotland has a few far-flung islands up North which can place them out of the initial map box.\n",
    "\n",
    "Our choropleth map was coloured using population data with a single colour gradient as we thought this would look cleaner and less distracting compared to using multiple colours. We wanted to display the bikes data points on each local authority as circles, and took two different approaches to placing these circles: one was to find the mean of latitude and longitude in our dataset, and the other was to approximate central coordinates for each authority manually (with the Power of Google Maps) which gave us more control on positining.\n",
    "\n",
    "Using both methods, we realised that as the coordinates in the DfT data set are based on positions on roads, the mean coordinates are heavily biased towards road clusters, which could have unintended or unexpected behaviour when positioning our circles. In that case, it worked out without incident.\n",
    "\n",
    "We also explored a number of different visual options for the choropleth map by changing 'tiles' (which impacts land and see colours) and finally decided upon the `CartoDB Positron` tiling theme. \n",
    "\n",
    "An interesting and initially difficult exercise was parsing the TopoJSON data (which naturally behaves like a dictionary as it is in JSON format) to pull out relevant authority names so we could use them as keys for our visualisation. As the JSON structure was not very intuitive, we had to run through multiple key extractions to finally arrive at the terribly-named `LAD13NM` which represents the name of the local authority.\n",
    "\n",
    "Ultimately, we were able to project circles located on centralised coordinates per local authority. We had to make some adjustments to the radius by using a scalar multiple of bike observations as the maximum value was 0.2, rendering the circles as mere dots. Thus, we scaled to a high magnitude for clearer visualisation. The reader can see the results and interact with the map below.\n",
    "\n",
    "## âš ï¸ TODO:\n",
    "* Use only 2019 bikes observation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant columns for mapviz\n",
    "map_cols = ['count_point_id', 'local_authority_name', 'count_date', 'road_name',\n",
    "            'hour', 'direction_of_travel', 'latitude', 'longitude', 'population']\n",
    "map_df = df_tidy[map_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually finding central lat and long of each local authority\n",
    "# Lat_longs data acquired from Google Maps\n",
    "auth_names = sorted(map_df.local_authority_name.unique())\n",
    "lat_longs = [[57.148499575046166, -2.0994503154974553],[57.273760025881934, -2.5166643080179645],[56.72768401017168, -2.9246922126651884],[56.09740006583508, -5.505829190157146],[55.95233877595505, -3.188860106840768],[56.14959079238047, -3.7457616021792624],[55.085097679793755, -3.9648777663637222],[56.46547771536677, -2.9702673126999213],[55.465299446108205, -4.307460244966752],[55.950679905225755, -4.21793961543706],[55.943997191276125, -2.7324635644378654],[55.75505725179119, -4.362197202163319],[58.24343874214309, -6.382783267363518],[56.00349190992439, -3.7933138794341983], [56.246895135851844, -3.087199713717091],[55.85890878015006, -4.246879570599694],[57.47148264549703, -4.918077719700579],[55.910241417028985, -4.73843337649973],[55.8380756961132, -3.091164797971764],[57.507658214268204, -3.2238412189671544], [55.72279452380807, -4.743315946865488], [55.87990377885493, -3.943898088843739], [59.02526102084918, -3.006499055095088], [56.55895608244633, -3.8058868668882613], [55.84937262539755, -4.534918906544666], [55.580570268563, -2.726163579854028], [60.33439464263681, -1.2344978915362006], [55.29083618369606, -4.6958972343105625], [55.59988380366663, -3.7749667034686474], [56.22513721244209, -4.338586907406815], [55.982771806368184, -4.5149571176594625], [55.88803749019023, -3.570496063254735]]\n",
    "coords_dict = dict(zip(auth_names, lat_longs)) \n",
    "coords_df = pd.DataFrame.from_dict(coords_dict, orient='index', columns=['cent_lat', 'cent_long'])\n",
    "coords_df.reset_index(level=0, inplace=True)\n",
    "coords_df.rename(columns={'index':'local_authority_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean lat and long of each local authority from DfT data\n",
    "coords_df_tidy = df_tidy[['local_authority_name', 'latitude', 'longitude']]\n",
    "coords_df_tidy = coords_df_tidy.groupby('local_authority_name')[['latitude', 'longitude']].mean()\n",
    "coords_df_tidy.rename(columns={'latitude':'cent_lat', 'longitude':'cent_long'}, inplace=True) # Renaming columns\n",
    "coords_df_tidy.reset_index(level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **Please note warning in code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full outer SQL join of map_df and coords_df_tidy\n",
    "# WARNING: this operation is idempotent so repeated runs will keep adding cent_lat and cent_long columns\n",
    "# Must restart kernel if this needs to be run again \n",
    "map_df = pd.merge(map_df, coords_df_tidy, on='local_authority_name', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching DfT authority names with external authority names\n",
    "# JSON file is of local authority borders for choropleth map\n",
    "# Line(s) 7: Martin Chorley, Bob Harper, Charles Boutaud\n",
    "# URL: https://github.com/martinjc/UK-GeoJSON\n",
    "# Accessed on 7th December 2020\n",
    "\n",
    "auth_geo = os.path.join(os.getcwd(), 'data/local_auth.json')\n",
    "with open(auth_geo) as f:\n",
    "    scotland_topo = json.load(f)\n",
    "topo_names = []\n",
    "for idx in range(32):\n",
    "    topo_names.append(scotland_topo['objects']['lad']['geometries'][idx]['properties']['LAD13NM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise map of Scotland\n",
    "scotland_bright = folium.Map(location=[57.8906712, -4.2026458], zoom_start=6.25, tiles='cartodbpositron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create choloropleth of population data\n",
    "# Code adapted from reference:\n",
    "# Line(s) 5-25: Amanda Iglasias Moreno\n",
    "# URL: https://towardsdatascience.com/choropleth-maps-with-folium-1a5b8bcdd392\n",
    "# Accessed on 7th December 2020\n",
    "choropleth = folium.Choropleth(geo_data = scotland_topo,              \n",
    "    topojson = 'objects.lad',\n",
    "    key_on ='feature.properties.LAD13NM',\n",
    "    data = map_df,\n",
    "    columns = ['local_authority_name', 'population'], \n",
    "    name ='choropleth',                                   \n",
    "    fill_color = 'Blues', \n",
    "    fill_opacity = 1, # Prev 0.7\n",
    "    line_opacity = 0.5, # Prev 0.2\n",
    "    highlight = True,\n",
    "    legend_name='Population',\n",
    "    smooth_factor=0,\n",
    ").add_to(scotland_bright) # Prev scotland\n",
    "\n",
    "# Add labels indicating local authority name\n",
    "# This will allow you to hover over any part of a local authority to see name\n",
    "style_function = \"font-size: 12px\"\n",
    "choropleth.geojson.add_child(\n",
    "    folium.features.GeoJsonTooltip(['LAD13NM'], style=style_function, labels=False))\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(scotland_bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **Please note warning in code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying up dedicated dataframe for our bikes visualisation\n",
    "bikes_viz_df = bikes_per_10k_residents.to_frame()\n",
    "bikes_viz_df.rename(columns={0:'bikes_10k'}, inplace=True)\n",
    "bikes_viz_df.reset_index(level=0, inplace=True) # Resetting index\n",
    "\n",
    "# WARNING: this operation is idempotent so repeated runs will keep adding cent_lat and cent_long columns\n",
    "# Must restart kernel if this needs to be run again \n",
    "bikes_viz_df = pd.merge(bikes_viz_df, coords_df_tidy, on='local_authority_name', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add circles whose sizes represent bikes per 10k residents\n",
    "# Code adapted from reference:\n",
    "# Line(s) 8-17: Sunny Solanki\n",
    "# URL: https://coderzcolumn.com/tutorials/data-science/interactive-maps-choropleth-scattermap-using-folium\n",
    "# Accessed: 7th December 2020 \n",
    "\n",
    "normalizer = max(bikes_viz_df['bikes_10k'])\n",
    "for auth_name in auth_names:\n",
    "    folium.Circle(\n",
    "        location = [bikes_viz_df.loc[bikes_viz_df['local_authority_name'] == auth_name]['cent_lat'].values[0], \n",
    "                bikes_viz_df.loc[bikes_viz_df['local_authority_name'] == auth_name]['cent_long'].values[0]],\n",
    "        radius = float(bikes_viz_df.loc[bikes_viz_df['local_authority_name'] == auth_name]['bikes_10k'].values[0]*7e4),\n",
    "        color = 'mediumseagreen',\n",
    "        fill_color = 'mediumseagreen',\n",
    "        popup = f\"Bike density per 10k residents: {round(bikes_viz_df.loc[bikes_viz_df['local_authority_name'] == auth_name]['bikes_10k'].values[0], 3)}\",\n",
    "        tooltip=auth_name\n",
    "    ).add_to(scotland_bright)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DfT 2019 Cycle Density per 10k Residents per Scottish Local Authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this to see final map\n",
    "scotland_bright"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: roads + vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We had a series of visualisations of the entire range of vehicles\n",
    "* In this one we wanted to see there was a difference in vehicle counts by month\n",
    "* We noticed that the trend was typically in the following order: ...\n",
    "* Across most months the trend in vehicle counts was consistent\n",
    "* What was interesting was the nominal number of counts seemed to be quite low for specific months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We hypothesised that DfT do not go out to look for specific vehicles (otherwise the trends would be quite different) but the reported counts are at least function of the different types of roads, given no other data\n",
    "* So we created a similar visualisation but faceted it with the type of road and noticed:\n",
    " * For the months where fewer vehicles were observed, for certain types of roads, no counts were recorded at all, and for others, there was significant variance over the years\n",
    " * This leads us to hypothesise the reported counts are also a function of availability of DfT staff - for example, July and August are typically school holidays so most working parents (and non-parents) are on vacation, and November is the lead up to Christmas (also it's cold!)\n",
    "* Note:\n",
    " * 278 A roads\n",
    " * 206 B roads\n",
    " * 9   M roads\n",
    " * Undefined C/U roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_road_letters = np.vectorize(lambda string: string[0])\n",
    "\n",
    "for road_counts in zip(*np.unique(extract_road_letters(df_tidy.road_name.unique()), return_counts = True)):\n",
    "    if road_counts[1] > 1:\n",
    "        print(f\"There are {road_counts[1]} {road_counts[0]} roads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining months for ordering purposes\n",
    "months = [\"March\", \"April\", \"May\", \"June\", \"July\", \"August\",\n",
    "          \"September\", \"October\", \"November\"]\n",
    "\n",
    "# Extract relevant columns for observing variation of counts\n",
    "# over the different months for different vehicles\n",
    "vehicle_cols = ['count_date', 'bikes', 'motorcycles', 'cars', 'buses', 'lgvs', 'hgvs']\n",
    "vehicle_df = df_tidy.copy()\n",
    "vehicle_df = vehicle_df[vehicle_cols]\n",
    "# Create new column for month name\n",
    "vehicle_df.loc[:,'month'] = vehicle_df['count_date'].dt.month_name()\n",
    "# Ordering month_df chronologically\n",
    "vehicle_df.loc[:,'month'] = pd.Categorical(vehicle_df['month'], categories=months, ordered=True)\n",
    "vehicle_df.sort_values(by='month', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "\n",
    "# Create the PairGrid\n",
    "vg = sns.PairGrid(vehicle_df.sort_values('cars', ascending=False),\n",
    "                 x_vars=vehicle_df.columns[1:-1], y_vars=['month'],\n",
    "                 height=10, aspect=.25)\n",
    "plt.subplots_adjust(top=.875)\n",
    "vg.fig.suptitle(\"DFT Distribution of Vehicles Counts by Month 2000-2019\",  \n",
    "                fontdict={'weight':'bold', 'color':'black'},\n",
    "                fontsize=30)\n",
    "\n",
    "# Draw a scatter plot using stripplot function\n",
    "vg.map(sns.stripplot, size=7.5, orient='h', marker='o',\n",
    "       palette='flare_r', linewidth=1, edgecolor='none',  alpha=.5)\n",
    "\n",
    "# Use semantically meaningful titles for the columns\n",
    "titles = [\"Bikes\", \"Motorcycles\", \"Cars\", \"Buses\",\n",
    "          \"LGVS\", \"HGVS\"]\n",
    "\n",
    "for ax, title in zip(vg.axes.flat, titles):\n",
    "    # Set a different title for each axes\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    \n",
    "    # Make the grid horizontal instead of vertical\n",
    "    ax.xaxis.grid(True)\n",
    "    ax.yaxis.grid(False)\n",
    "\n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns for observing variation of counts\n",
    "# over the different months for different road_types\n",
    "roads_cols = ['count_date', 'road_type']\n",
    "roads_df = df_tidy[roads_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivoting data wider so M, A, B, C, U become columns\n",
    "roads_df = roads_df.pivot_table(index=['count_date'], columns=['road_type'], aggfunc=len, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roads_df.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making type_df chronologically ordered\n",
    "roads_df['month'] = roads_df['count_date'].dt.month_name()\n",
    "roads_df['month'] = pd.Categorical(roads_df['month'], categories=months, ordered=True)\n",
    "roads_df.sort_values(by='month', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering type_df columns\n",
    "roads_cols = ['count_date', 'month', 'M', 'A', 'B', 'C', 'U']\n",
    "roads_df = roads_df.reindex(columns=roads_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid', font_scale=1.5)\n",
    "# Create the PairGrid\n",
    "roads_g = sns.PairGrid(roads_df.sort_values('A', ascending=False),\n",
    "                 x_vars=roads_df.columns[2:], y_vars=['month'],\n",
    "                 height=10, aspect=.25)\n",
    "plt.subplots_adjust(top=.835)\n",
    "roads_g.fig.suptitle(\"DFT Distribution of Road Type Counts \\nby Month 2000-2019\",  \n",
    "                fontdict={'weight':'bold', 'color':'black'},\n",
    "                fontsize=30)\n",
    "\n",
    "\n",
    "# Draw a scatter plot using stripplot function\n",
    "roads_g.map(sns.barplot, palette=\"flare_r\")\n",
    "\n",
    "# Use semantically meaningful titles for the columns\n",
    "titles = [\"M\", \"A\", \"B\", \"C\", \"U\"]\n",
    "\n",
    "for ax, title in zip(roads_g.axes.flat, titles):\n",
    "    # Set a different title for each axes\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    # Make the grid horizontal instead of vertical\n",
    "    ax.xaxis.grid(False)\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    \n",
    "sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~Q1: Bar chart of [X] longest roads showing start and end locations~\n",
    "\n",
    "### Method\n",
    "* Filter by distinguishing road name \n",
    "* Ignore U and C roads \n",
    "* We have start and end junctions (general locations)\n",
    "* How do we calculate distance?\n",
    "    1. Add up link lengths\n",
    "    2. Verify with Google Maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~Q2: Local authorities managing number of roads - mapviz - fixed point in time~\n",
    "\n",
    "### Outcome\n",
    "* 2019 map of roads managed\n",
    "* Static map of Scotland with gradient colour scheme\n",
    "\n",
    "### Method\n",
    "* Group by `local_authority_name`, `count_id`\n",
    "* Break down total number of roads per authority into `road_names`\n",
    "* Possible weight matrix to give larger, busier roads a larger influence in the map - don't want to give U and C thes same weighting as M and A roads\n",
    "* In `road_name` variable, cut string such that it's only a single character\n",
    "\n",
    "### Stretch and Challenge\n",
    "* Maintenance data to observe which authority spends the most on maintainance\n",
    "\n",
    "### Extra Packages:\n",
    "* folium\n",
    "* geopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Q3: Interactive visualization of the usage of Cycles on motorways in Scotland ~distribution of vehicles over each type of road over the past [X] years~ from 2000 to 2019\n",
    "\n",
    "### Outcome\n",
    "* x-axis: discrete vehicle type data e.g.  cars, bikes, buses, hgvs, lgvs etc\n",
    "* y-axis: type of road e.g. M, A, B, C, U\n",
    "* Scatter point for each category where size indicates prevalence of that type of vehicle on that type of road\n",
    "* Slider which shows variation of data over [X] years\n",
    "\n",
    "\n",
    "### Method\n",
    "* Group C and U roads\n",
    "* Normalize scatter points by \\pi*R^2\n",
    "\n",
    "\n",
    "### Stretch and Challenge\n",
    "* We have dotted line outline of average of previous 5 years giving the reader some indication of growth/reduction\n",
    "* x-axis: as emojis\n",
    "\n",
    "\n",
    "### Extra packages\n",
    "* plotly - used to vary the datapoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Out of curiosity we wanted to see the usage of HGVs on different types of roads (we hypothesised that it would be unlikely to see them on U/C roads - or at least very few of them)\n",
    "* To do this we tried a different map set - a shape file of the road network in Scotland\n",
    "* We filtered the data based on HGVs, and only retained rows where at least one HGV was observed\n",
    "* We had to use the `Shapely` library to convert the coordinates into a tuple-esque coordinate object, and then  plotted the points using `GeoPandas` \n",
    "* The resulting visualisation wasn't as scandalous as we would have liked so we thought out a different case: would you ever expect to see cycles on Motorways (which is illegal)?\n",
    "* It turned out that a particular stretch of some motorways saw a particularly large number of cycles observed (for a motorway and given the illegality, anything more than 0 counts as large!):\n",
    " * M8 on 11/06/00\n",
    " * M90 on 11/04/00 and 23/06/00\n",
    "* Desite our best efforts, we could not find any news or events to corroborate this (this was one article regarding roadworks for M80 where the speed limit was dropped to 40mph) so it either remains a mystery or a massive error (which is very unlikely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique roads, because we can\n",
    "unique_motorways = df_tidy.loc[df_tidy.road_type == 'M'].road_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting HGV-only data\n",
    "bikem_cols = ['count_date', 'hour', 'road_name', 'road_type', 'latitude', 'longitude', 'bikes']\n",
    "bikem_df = df_tidy[bikem_cols]\n",
    "bikem_df = bikem_df.loc[bikem_df['bikes'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bikem_df.loc[(bikem_df['road_type']=='M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Point objects for plotting HGV counts and their locations\n",
    "# on gpd map\n",
    "geometry = [Point(xy) for xy in zip(bikem_df['longitude'], bikem_df['latitude'])]\n",
    "# Specifying coordinate reference system\n",
    "crs = {'init': 'EPSG:4326'}\n",
    "bikem_df.drop(['latitude', 'longitude'], inplace=True, axis = 1) # drop unnessecary cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = gpd.GeoDataFrame(bikem_df, crs = crs, geometry = geometry) # Create geodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in external shape file of Scottish roads\n",
    "scotland_roads = gpd.read_file('data/roads.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scotland_auth_shape = gpd.read_file('pub_las.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize = (12, 20))\n",
    "scotland_roads.plot(ax = axis, color = 'gray', alpha = 0.2)\n",
    "#scotland_auth_shape.plot(ax = axis, color = 'white', edgecolor = 'black')\n",
    "#geo_df[geo_df['road_type'] == 'U'].plot(ax = axis, markersize = 15, color = 'red', marker = 'o', label = 'U')\n",
    "#geo_df[geo_df['road_type'] == 'C'].plot(ax = axis, markersize = 15, color = 'green', marker = 'o', label = 'C')\n",
    "geo_df[geo_df['road_type'] == 'M'].plot(ax = axis, markersize = 200, color = 'firebrick', marker = 'x', label = 'M')\n",
    "plt.legend(prop = {'size': 15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Time of day viz?\n",
    "\n",
    "### Outcome\n",
    "* x-axis: Hours of the day (07:00 - 18:00)\n",
    "* y-axis-1: `all_vehicles`\n",
    "* y-axis-2: types of vehicle (faceted plot)\n",
    "* Light alpha filled densities\n",
    "\n",
    "### Method\n",
    "* Get hours day\n",
    "* Group vehicle count by hour type\n",
    "* Further subset those counts by vehicle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Answering one of the provided questions, we wanted a nice-looking distribution plot of vehicle counts per hour of day\n",
    "* We decided to use the scheme and some formatting from the `FiveThirtyEight` theme\n",
    "* As this is a graph we expect many people to provide, we wanted to make it look a little snazzier by directly annotating the vehicle type on the graph itself in a visually engaging way\n",
    "* Observations:\n",
    " * Follows an expected trend with peaks in the morning (commuter traffic) and evening\n",
    " * Typically more observations at the end of day suggesting it is a mix of commuter and leisure traffic (going out for the evening after work)\n",
    " * HGVs bucked this trend which we surmise because their work involves driving them to and from depots and distribution centres during working hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tod_df = df_tidy[['hour'] + new_names] # Extracting relevant variables\n",
    "tod_df = tod_df.groupby('hour').sum()  # Find total counts of each vehicle at every hour\n",
    "tod_df.reset_index(level=0, inplace=True) # Index with numbers not names\n",
    "tod_df = tod_df.melt(id_vars='hour', var_name='vehicle_type') # Pivot longer\n",
    "tod_df.vehicle_type = tod_df.vehicle_type.astype('category')  # Converting datatype to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant data to plot\n",
    "x = tod_df.hour.unique()\n",
    "y1 = tod_df.loc[tod_df.vehicle_type=='cars']['value']\n",
    "y2 = tod_df.loc[tod_df.vehicle_type=='lgvs']['value']\n",
    "y3 = tod_df.loc[tod_df.vehicle_type=='hgvs']['value']\n",
    "y4 = tod_df.loc[tod_df.vehicle_type=='buses']['value']\n",
    "y5 = tod_df.loc[tod_df.vehicle_type=='motorcycles']['value']\n",
    "y6 = tod_df.loc[tod_df.vehicle_type=='bikes']['value']\n",
    "y = [y1, y2, y3, y4, y5, y6] # Amalgamate into single list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"CARS\", \"LGVS\", \"HGVS\", \"BUSES\", \"MOTORBIKES\", \"BIKES\"]\n",
    "xticks_ = np.arange(7, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "# Plotting facetgrid manually\n",
    "fig, axes = plt.subplots(6, 1, sharex=False, sharey=False, figsize=(16, 32), facecolor='white')\n",
    "plt.subplots_adjust(top=.94)\n",
    "\n",
    "fig.suptitle(\"DFT Distribution of Vehicles Counts \\nby Hour 2000-2019\",  \n",
    "                fontdict={'weight':'bold', 'color':'black'},\n",
    "                fontsize=40)\n",
    "hues = ['#e5ae38', '#8b8b8b', '#6d904f', '#fc4f30', '#810f7c', '#008fd5']\n",
    "for i, ax in enumerate(axes):\n",
    "    sns.lineplot(data=tod_df, x='hour', y=y[i], ax=ax, hue='vehicle_type')\n",
    "    ax.fill_between(x, y[i], color=hues[i], alpha=0.35)\n",
    "    ax.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xticks(xticks_),\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f')),\n",
    "    ax.get_legend().remove()\n",
    "    t = ax.annotate(f\"{labels[i]}\", \n",
    "                xy=(10,10), \n",
    "                xycoords='data', \n",
    "                textcoords='axes fraction', \n",
    "                xytext=(.94,.075),\n",
    "                ha='right',\n",
    "                fontsize=60,\n",
    "                fontweight='bold',\n",
    "                color='white')\n",
    "    t.set_path_effects([path_effects.Stroke(linewidth=5, foreground=hues[i]),\n",
    "                       path_effects.Normal()])\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.xaxis.grid(True)\n",
    "    ax.yaxis.grid(False)\n",
    "    sns.despine(left=True, bottom=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs for Q4:\n",
    "\n",
    "* Add all hours to x-axis\n",
    "* Format y-axis, and change labels\n",
    "* Add hover on hour points to read values\n",
    "* Display legend of colour <--> vehicle_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Bikes observed per 10k residents / capita per local authority \n",
    "\n",
    "### Caveats\n",
    "1. Count data of bikes is assumed to be representative of the prevalence of bikes in the authority\n",
    "2. A bike count within a local authority maps to that bike actually belonging to that authority\n",
    "3. Double counting effects are ignored\n",
    "\n",
    "This is because bikes are not registered vehicles and hence this assumption has to be made. We think this is a reasonable assumption since bikes are used for micro-mobility in the majority of cases.\n",
    "\n",
    "### Outcome\n",
    "* Horizontal bar chart ordered in descending order\n",
    "\n",
    "### Method\n",
    "* Get population data and store in tidy dataframe\n",
    "* Plot the data\n",
    "\n",
    "## Additional\n",
    "* Encode or visualise population data\n",
    "* Bikes per 10,000 residents - same as log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
